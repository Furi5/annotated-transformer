{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 写一个简单的 transformer 框架\n",
    "source: https://fancyerii.github.io/2019/03/09/transformer-codes/\n",
    "\n",
    "没有特别看懂，找了另一个微信上的推送，写的比较明白！\n",
    "https://mp.weixin.qq.com/s/QdaRMbYLAENBfG-wiB4eEA\n",
    "\n",
    "1. 弄清楚transformer 模块\n",
    "- Embedding: Context Embedding, Positional Emdedding\n",
    "- Encoder: Multi head Attention, Feed Forward, layerNorm, residual connection\n",
    "- Decoder: Masked Muti head Attention, Feed Forward, layerNorm, residual connection\n",
    "- Output: Linear + Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 6\n",
    "        self.d_model = 20\n",
    "        self.n_heads = 2\n",
    "\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        self.dim_k  = self.d_model % self.n_heads\n",
    "        self.dim_v = self.d_model % self.n_heads\n",
    "\n",
    "        self.padding_size = 30\n",
    "        self.UNK = 5\n",
    "        self.PAD = 4\n",
    "\n",
    "        self.N = 6\n",
    "        self.p = 0.1\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Embedding的函数\n",
    "- context embedding\n",
    "- positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,config.d_model,padding_idx=config.PAD)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i]) < config.padding_size:\n",
    "                x[i].extend([config.PAD]*(config.padding_size-len(x[i])))\n",
    "            else:\n",
    "                x[i] = x[i][:config.padding_size]\n",
    "        x = self.embedding(torch.tensor(x))\n",
    "        return x\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self,seq_len,embedding_dim):\n",
    "        positional_encoding = np.zeros((seq_len,embedding_dim))\n",
    "        for pos in range(positional_encoding.shape[0]):\n",
    "            for i in range(positional_encoding.shape[1]):\n",
    "                positional_encoding[pos][i] = math.sin(pos/(10000**(2*i/self.d_model))) if i % 2 == 0 else math.cos(pos/(10000**(2*i/self.d_model)))\n",
    "        return torch.from_numpy(positional_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Encoder 的函数: \n",
    "- multi head attention\n",
    "- Feed forward\n",
    "- layerNorm\n",
    "- residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mutihead_Attention(nn.Module):\n",
    "    def __init__(self,d_model,dim_k,dim_v,n_heads):\n",
    "        super(Mutihead_Attention, self).__init__()\n",
    "        self.dim_v = dim_v\n",
    "        self.dim_k = dim_k\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.q = nn.Linear(d_model,dim_k)\n",
    "        self.k = nn.Linear(d_model,dim_k)\n",
    "        self.v = nn.Linear(d_model,dim_v)\n",
    "\n",
    "        self.o = nn.Linear(dim_v,d_model)\n",
    "        self.norm_fact = 1 / math.sqrt(d_model)\n",
    "\n",
    "    def generate_mask(self,dim):\n",
    "        # 此处是 sequence mask ，防止 decoder窥视后面时间步的信息。\n",
    "        # padding mask 在数据输入模型之前完成。\n",
    "        matirx = np.ones((dim,dim))\n",
    "        mask = torch.Tensor(np.tril(matirx))\n",
    "\n",
    "        return mask==1\n",
    "\n",
    "    def forward(self,x,y,requires_mask=False):\n",
    "        assert self.dim_k % self.n_heads == 0 and self.dim_v % self.n_heads == 0\n",
    "        # size of x : [batch_size * seq_len * batch_size]\n",
    "        # 对 x 进行自注意力\n",
    "        Q = self.q(x).reshape(-1,x.shape[0],x.shape[1],self.dim_k // self.n_heads) # n_heads * batch_size * seq_len * dim_k\n",
    "        K = self.k(x).reshape(-1,x.shape[0],x.shape[1],self.dim_k // self.n_heads) # n_heads * batch_size * seq_len * dim_k\n",
    "        V = self.v(y).reshape(-1,y.shape[0],y.shape[1],self.dim_v // self.n_heads) # n_heads * batch_size * seq_len * dim_v\n",
    "        # print(\"Attention V shape : {}\".format(V.shape))\n",
    "        attention_score = torch.matmul(Q,K.permute(0,1,3,2)) * self.norm_fact\n",
    "        if requires_mask:\n",
    "            mask = self.generate_mask(x.shape[1])\n",
    "            attention_score.masked_fill(mask,value=float(\"-inf\")) # 注意这里的小Trick，不需要将Q,K,V 分别MASK,只MASKSoftmax之前的结果就好了\n",
    "        output = torch.matmul(attention_score,V).reshape(y.shape[0],y.shape[1],-1)\n",
    "        # print(\"Attention output shape : {}\".format(output.shape))\n",
    "        output = self.o(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class Feed_Forward(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim=2048):\n",
    "        super(Feed_Forward, self).__init__()\n",
    "        self.L1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.L2 = nn.Linear(hidden_dim,input_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = nn.ReLU()(self.L1(x))\n",
    "        output = self.L2(output)\n",
    "        return output\n",
    "\n",
    "class Add_Norm(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.dropout = nn.Dropout(config.p)\n",
    "        super(Add_Norm, self).__init__()\n",
    "\n",
    "    def forward(self,x,sub_layer,**kwargs):\n",
    "        sub_output = sub_layer(x,**kwargs)\n",
    "        # print(\"{} output : {}\".format(sub_layer,sub_output.size()))\n",
    "        x = self.dropout(x + sub_output)\n",
    "\n",
    "        layer_norm = nn.LayerNorm(x.size()[1:])\n",
    "        out = layer_norm(x)\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(config.d_model)\n",
    "        self.muti_atten = Mutihead_Attention(config.d_model,config.dim_k,config.dim_v,config.n_heads)\n",
    "        self.feed_forward = Feed_Forward(config.d_model)\n",
    "\n",
    "        self.add_norm = Add_Norm()\n",
    "\n",
    "    def forward(self,x): # batch_size * seq_len 并且 x 的类型不是tensor，是普通list\n",
    "        x += self.positional_encoding(x.shape[1],config.d_model)\n",
    "        # print(\"After positional_encoding: {}\".format(x.size()))\n",
    "        output = self.add_norm(x,self.muti_atten,y=x)\n",
    "        output = self.add_norm(output,self.feed_forward)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(config.d_model)\n",
    "        self.muti_atten = Mutihead_Attention(config.d_model,config.dim_k,config.dim_v,config.n_heads)\n",
    "        self.feed_forward = Feed_Forward(config.d_model)\n",
    "        self.add_norm = Add_Norm()\n",
    "\n",
    "    def forward(self,x,encoder_output): # batch_size * seq_len 并且 x 的类型不是tensor，是普通list\n",
    "        # print(x.size())\n",
    "        x += self.positional_encoding(x.shape[1],config.d_model)\n",
    "        # print(x.size())\n",
    "        # 第一个 sub_layer\n",
    "        output = self.add_norm(x,self.muti_atten,y=x,requires_mask=True)\n",
    "        # 第二个 sub_layer\n",
    "        output = self.add_norm(output,self.muti_atten,y=encoder_output,requires_mask=True)\n",
    "        # 第三个 sub_layer\n",
    "        output = self.add_norm(output,self.feed_forward)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer中堆叠了6个我们上文中实现的Encoder 和 Decoder。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer_layer, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_input,x_output = x\n",
    "        encoder_output = self.encoder(x_input)\n",
    "        decoder_output = self.decoder(x_output,encoder_output)\n",
    "        return decoder_output\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,N,vocab_size,output_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding_input = Embedding(vocab_size=vocab_size)\n",
    "        self.embedding_output = Embedding(vocab_size=vocab_size)\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.linear = nn.Linear(config.d_model,output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.model = nn.Sequential(*[Transformer_layer() for _ in range(N)])\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_input , x_output = x\n",
    "        x_input = self.embedding_input(x_input)\n",
    "        x_output = self.embedding_output(x_output)\n",
    "\n",
    "        _ , output = self.model((x_input,x_output))\n",
    "\n",
    "        output = self.linear(output)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
